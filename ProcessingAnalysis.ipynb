{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24eccf20-3fc0-4464-9da8-a77af2d88429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from datetime import datetime\n",
    "\n",
    "#Get the data\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a338d4b-4718-4541-ade1-a436b9d5420a",
   "metadata": {},
   "source": [
    "<h4>Create Days Since connected Column</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1f3ad0-c17a-4d3d-8901-cf41fd19dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = pd.to_datetime(df[\"Connect_Date\"], format = '%d/%m/%y')\n",
    "date_col.astype('int64')\n",
    "df[\"today\"] = pd.Timestamp.today()\n",
    "df[\"Days_since_connected\"] = df[\"today\"]-date_col\n",
    "df[\"Days_since_connected\"] = df[\"Days_since_connected\"].dt.days\n",
    "df.drop([\"today\", \"Connect_Date\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387e82d2-6535-46ec-809f-c25ccc04dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove id\n",
    "df.drop([\"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7664b15-09cb-4d5f-9c26-730944f644e7",
   "metadata": {},
   "source": [
    "<h4>Create continuous and categorical columns</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee450ca-a1d4-45bf-8887-f7063d302ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous=df._get_numeric_data().columns.tolist()\n",
    "continuous.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f99eced-efe6-40b2-bda5-c3d21c818f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical=df.select_dtypes(include=['object', 'bool', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fd9b1-30c8-4b91-bce1-17fb300edf21",
   "metadata": {},
   "source": [
    "<h4> Create test and training sets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede589d1-bdc3-426c-9eb7-b76b96f322fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['target']\n",
    "X = df.drop(['target'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769a7bb-387b-4204-b916-6d41760c6029",
   "metadata": {},
   "source": [
    "<h4> Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379a5846-4c9d-427b-b4b1-91fce416b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#Something Worked!!! This leads to significant improvement in top 20 metric\n",
    "#This creates a weigting array that assigns much higher weight \n",
    "#to the instances that churn and have average cost min higher than a certain\n",
    "#threshold. sample_weights are the used to fit the model.\n",
    "#The threshold 0.179141 is the top quartile of average cost min\n",
    "#750 is just arbitraray and can be tuned.\n",
    "ind1 = np.array(X_train[\"average cost min\"] >= 0.179141)\n",
    "ind2 = np.array(y_train)\n",
    "ind = ind1*ind2\n",
    "sample_weights = (ind*750)+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Save untransformed data for later\n",
    "X_train_raw = X_train.copy()\n",
    "X_test_raw = X_test.copy()\n",
    "\n",
    "#Cotinuous Transformations\n",
    "cts_pipe = Pipeline([\n",
    "    ('ImputeContinuous', SimpleImputer(strategy=\"median\")),\n",
    "    ('StandardScaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "#Categorical Transformations\n",
    "cat_pipe = Pipeline([\n",
    "    ('OneHot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "#Apply to columns\n",
    "t = ColumnTransformer(\n",
    "    [\n",
    "    (\"cts\", cts_pipe, continuous),\n",
    "    (\"cat\", cat_pipe, categorical)\n",
    "    ])\n",
    "\n",
    "# All trasnformations. Add any here that would apply to both continuous and categorical\n",
    "final_pipeline = Pipeline([\n",
    "    ('columns', t),\n",
    "    #('PCA', PCA())\n",
    "    ])\n",
    "\n",
    "\n",
    "X_train = final_pipeline.fit_transform(X_train)\n",
    "X_test= final_pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6199947-b32a-40e4-ac38-4dc1c90111f9",
   "metadata": {},
   "source": [
    "<h4>Function to compute top 20 Metric</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8752a988-c0c7-47b1-a683-c10c98d0c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that calculates the sum of avg_cost_min with the highest predicted probabilities\n",
    "#prediction is the predicted probabilities from the model.\n",
    "#The argument testSet should be the unstandardized testset\n",
    "def avgCostSum(prediction, y_testSet, X_testSet):\n",
    "    #Get an array of sorted predictions in descending order\n",
    "    index = np.argsort(prediction)[::-1]\n",
    "    #Select the sorted avg_cost_min column from the unstandardized dataframe, then get the 20 highest\n",
    "    avg_cost_min = X_testSet.iloc[index,].join(y_testSet.iloc[index])\n",
    "    avg_cost_min = avg_cost_min[[\"average cost min\", \"target\"]][0:20]\n",
    "    #Return the sum of the top 20 for those that were correctly predicted\n",
    "    return avg_cost_min[avg_cost_min[\"target\"]==1][\"average cost min\"].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3658f9-c2cd-4c51-b123-9901f71a639b",
   "metadata": {},
   "source": [
    "<h4> Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69987fb2-2791-47e0-a0cb-3d8d9b10965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------AUC----------Top20------Accuracy----Precision----Recall\n",
      "LR  :    0.836281    3.837880    0.731417    0.336111    0.790850\n",
      "LRW :    0.857851    3.924467    0.648167    0.290456    0.915033\n",
      "RF  :    0.908512    1.902645    0.895937    0.740000    0.483660\n",
      "RFW :    0.915063    2.373663    0.887017    0.746835    0.385621\n",
      "DT  :    0.783115    2.130154    0.887017    0.625806    0.633987\n",
      "DTW :    0.775063    2.164788    0.887017    0.630872    0.614379\n",
      "GB  :    0.867074    3.820324    0.845391    0.493671    0.764706\n",
      "XGB :    0.939596    2.139042    0.909812    0.731343    0.640523\n",
      "XGBW:    0.936229    2.219334    0.920714    0.732484    0.751634\n",
      "GNB:    0.674711    4.647400    0.731417    0.295139    0.555556\n",
      "CNNs:    0.872839    2.445988    0.900892    0.680272    0.653595\n",
      "SVM:    0.909940    2.605811    0.907830    0.717391    0.647059\n",
      "Stacking:    0.927440    2.703068    0.151635    0.151635    1.000000\n"
     ]
    }
   ],
   "source": [
    "#My version test\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "models = []\n",
    "models.append(('LR  ', LogisticRegression(solver='lbfgs', max_iter=1000)))\n",
    "models.append(('LRW ', LogisticRegression(solver='lbfgs', max_iter=1000, class_weight=\"balanced\")))\n",
    "models.append(('RF  ', RandomForestClassifier()))\n",
    "models.append(('RFW ', RandomForestClassifier(class_weight=\"balanced\")))\n",
    "models.append(('DT  ', DecisionTreeClassifier(class_weight=\"balanced\")))\n",
    "models.append(('DTW ', DecisionTreeClassifier(class_weight=\"balanced\")))\n",
    "models.append(('GB  ', GradientBoostingClassifier(n_estimators = 100, learning_rate =0.2, max_depth=3, random_state=42)))\n",
    "models.append(('XGB ', XGBClassifier()))\n",
    "models.append(('XGBW', XGBClassifier(scale_pos_weight=6)))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('CNNs', MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000, alpha=1e-6,\n",
    "                        solver='adam', tol=1e-6, random_state=1,\n",
    "                        learning_rate_init=.1)))\n",
    "models.append(('SVM', SVC(kernel='rbf', C=5.0, probability=True)))\n",
    "b_c=[\n",
    "    ('RFW ', RandomForestClassifier(class_weight=\"balanced\")),\n",
    "    ('SVM', SVC(kernel='rbf', C=5.0, probability=True)),\n",
    "    ('XGBW', XGBClassifier(scale_pos_weight=6))\n",
    "]\n",
    "m_c= LogisticRegression()\n",
    "models.append(('Stacking', StackingClassifier(estimators=b_c, final_estimator=m_c, cv=5)))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "print(\"---------AUC----------Top20------Accuracy----Precision----Recall\")\n",
    "for name, model in models:\n",
    "    if hasattr(model, 'fit') and 'sample_weight' in model.fit.__code__.co_varnames:\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)  # Fit without sample weights\n",
    "    \n",
    "    #model.fit(X_train, y_train, sample_weight = sample_weights)\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    cat_preds = [round(value) for value in y_pred]\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, cat_preds)\n",
    "    precision = precision_score(y_test,cat_preds)\n",
    "    recall = recall_score(y_test,cat_preds)\n",
    "    top20 = avgCostSum(y_pred, y_test, X_test_raw)\n",
    "    results.append(auc)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s:    %f    %f    %f    %f    %f\" % (name, auc, top20, accuracy, precision, recall)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd1387-cf9a-4db4-a585-94581cb63f90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ca80e83-b4e8-4dbd-9405-7e30be212110",
   "metadata": {},
   "source": [
    "<h4> Try oversampling Left out for now. Don't know how to implement with the sample_weights.\n",
    "Might not be necesarry since most algorithms have a class_weight or scale_pos_weight option"
   ]
  },
  {
   "cell_type": "raw",
   "id": "893cd83d-a013-4318-936b-e9b410458245",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE(sampling_strategy=0.5,random_state=42)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "869f19b0-0e3f-4d64-803a-9ec713be6625",
   "metadata": {},
   "source": [
    "results = []\n",
    "names = []\n",
    "print(\"---------AUC----------Top20------Accuracy----Precision----Recall\")\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    cat_preds = [round(value) for value in y_pred]\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, cat_preds)\n",
    "    precision = precision_score(y_test,cat_preds)\n",
    "    recall = recall_score(y_test,cat_preds)\n",
    "    top20 = avgCostSum(y_pred, y_test, X_test_raw)\n",
    "    results.append(auc)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s:    %f    %f    %f    %f    %f\" % (name, auc, top20, accuracy, precision, recall)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb933da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
