{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca0db5-1ef1-4acb-a56f-f825448dbbf7",
   "metadata": {},
   "source": [
    "# Test over many different random sampels. Used for Hyperparameter tuning and comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d26bbac-362c-4bee-85ce-269787157deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from catboost import CatBoostClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566c8f8e-1f1f-4bcc-9184-316081ecabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the sum of avg_cost_min with the highest predicted probabilities\n",
    "# prediction is the predicted probabilities from the model.\n",
    "# The argument testSet should be the unstandardized testset\n",
    "\n",
    "def avgCostSum(prediction, y_testSet, X_testSet):\n",
    "    #Get an array of sorted predictions in descending order\n",
    "    index = np.argsort(prediction)[::-1]\n",
    "    #Select the sorted avg_cost_min column from the unstandardized dataframe, then get the 20 highest\n",
    "    avg_cost_min = X_testSet.iloc[index,].join(y_testSet.iloc[index])\n",
    "    avg_cost_min = avg_cost_min[[\"average cost min\", \"target\"]][0:20]\n",
    "    #Return the sum of the top 20 for those that were correctly predicted\n",
    "    return avg_cost_min[avg_cost_min[\"target\"]==1][\"average cost min\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52455706-79d2-42ce-9fe4-e0759345ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/assignment1_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55939606-306e-4817-9461-ec857a0aaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop the column id: it is unique to each customer and has no predictive value\n",
    "assert len(df.id.unique()) == len(df)\n",
    "df_clean = df.drop([\"id\"], axis=1)\n",
    "\n",
    "# Convert Connect date into number of days\n",
    "date_col = pd.to_datetime(df_clean[\"Connect_Date\"], format = '%d/%m/%y')\n",
    "date_col.astype('int64')\n",
    "df_clean[\"today\"] = pd.Timestamp.today()\n",
    "df_clean[\"Days_since_connected\"] = df_clean[\"today\"]-date_col\n",
    "df_clean[\"Days_since_connected\"] = df_clean[\"Days_since_connected\"].dt.days\n",
    "df_clean.drop([\"today\", \"Connect_Date\"], axis=1, inplace=True)\n",
    "\n",
    "# Set the target as categorical\n",
    "#df_clean['target'] = df_clean['target'].map({0:'retained', 1:'churned'}).astype('object')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fc7e69-35f5-4993-9ff2-8fd4661ad100",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean['target']\n",
    "X = df_clean.drop(['target'], axis=1, inplace=False)\n",
    "\n",
    "continuous = df_clean._get_numeric_data().columns.drop('target').tolist()\n",
    "categorical = df_clean.select_dtypes(include=['object', 'bool', 'category']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04097ab-93e8-40af-9a84-a0037770286c",
   "metadata": {},
   "source": [
    "## The Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2340307-a581-4d27-91b2-160aa1b7f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Actual thresh: 0.217383\n",
      "predicted thresh_high: 0.32293410000000006\n",
      "predicted thresh_med: 0.18559240000000002\n",
      "iteration 1\n",
      "Actual thresh: 0.233411\n",
      "predicted thresh_high: 0.32506285000000007\n",
      "predicted thresh_med: 0.1858592\n",
      "iteration 2\n",
      "Actual thresh: 0.250189\n",
      "predicted thresh_high: 0.3514979000000006\n",
      "predicted thresh_med: 0.18635300000000002\n",
      "iteration 3\n",
      "Actual thresh: 0.222783\n",
      "predicted thresh_high: 0.3349249\n",
      "predicted thresh_med: 0.1858592\n",
      "iteration 4\n",
      "Actual thresh: 0.231511\n",
      "predicted thresh_high: 0.33462590000000003\n",
      "predicted thresh_med: 0.1860228\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>top20</th>\n",
       "      <th>%oftop20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RFW</th>\n",
       "      <td>0.637625</td>\n",
       "      <td>5.824797</td>\n",
       "      <td>0.769553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBW</th>\n",
       "      <td>0.774897</td>\n",
       "      <td>5.403261</td>\n",
       "      <td>0.714785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost</th>\n",
       "      <td>0.761704</td>\n",
       "      <td>5.592907</td>\n",
       "      <td>0.735807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               auc     top20  %oftop20\n",
       "name                                  \n",
       "RFW       0.637625  5.824797  0.769553\n",
       "XGBW      0.774897  5.403261  0.714785\n",
       "catboost  0.761704  5.592907  0.735807"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=[]\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"iteration\", i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=4*i)\n",
    "    \n",
    "    \n",
    "    # Save untransformed data for later\n",
    "    X_train_raw = X_train.copy()\n",
    "    X_test_raw = X_test.copy()\n",
    "    \n",
    "    # Continuous Transformations\n",
    "    cts_pipe = Pipeline([\n",
    "        ('ImputeContinuous', SimpleImputer(strategy=\"median\")),\n",
    "        #('StandardScaler', StandardScaler())\n",
    "        ])\n",
    "    \n",
    "    # Categorical Transformations\n",
    "    cat_pipe = Pipeline([\n",
    "        ('OneHot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "    \n",
    "    # Apply to columns\n",
    "    t = ColumnTransformer(\n",
    "        [\n",
    "        (\"cts\", cts_pipe, continuous),\n",
    "        (\"cat\", cat_pipe, categorical)\n",
    "        ])\n",
    "    \n",
    "    # All trasnformations. Add any here that would apply to both continuous and categorical\n",
    "    final_pipeline = Pipeline([\n",
    "        ('columns', t),\n",
    "        #('PCA', PCA())\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    X_train = final_pipeline.fit_transform(X_train)\n",
    "    X_test= final_pipeline.transform(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #maximum possible value for top 20 score\n",
    "    x = X_test_raw.join(y_test)\n",
    "    #Save the top 20 threshhold for later\n",
    "    top_20_thresh = (x[x[\"target\"] ==1 ][[\"average cost min\", \"target\"]].sort_values(by = \"average cost min\", ascending = False)[0:20].min().iloc[0])\n",
    "    #Save the sum of the top 20\n",
    "    x = x[x[\"target\"] ==1 ][[\"average cost min\", \"target\"]].sort_values(by = \"average cost min\", ascending = False)[0:20].sum()\n",
    "    max_target=(x.iloc[0])\n",
    "    print(\"Actual thresh:\", top_20_thresh)\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    percentile = 90\n",
    "    avg_cost_thresh = np.percentile(X_train_raw[\"average cost min\"], percentile)\n",
    "    print(\"predicted thresh:\", avg_cost_thresh)\n",
    "   \n",
    "    \n",
    "    ###############\n",
    "    high_model_weight = 0.8\n",
    "    reg_model_weight = 1-high_model_weight\n",
    "    y_train_high = y_train*(X_train_raw[\"average cost min\"]>=avg_cost_thresh)\n",
    "    \n",
    "    #################\n",
    "    \n",
    "    #combining the above with oversampling improves top 20 score\n",
    "    oversample = SMOTE(random_state=42)\n",
    "    X_train_high, y_train_high = oversample.fit_resample(X_train, y_train_high)\n",
    "    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #restrict to fewer models for faster testing\n",
    "    models = []\n",
    "    models.append(('RFW ', RandomForestClassifier(class_weight=\"balanced\",\n",
    "                                                  n_estimators=50,\n",
    "                                                  max_features=\"sqrt\",\n",
    "                                                 criterion=\"entropy\",\n",
    "                                                  random_state=42)))\n",
    "\n",
    "    models.append(('XGBW', XGBClassifier(scale_pos_weight=6, learning_rate=0.3)))   \n",
    "    \n",
    "    models.append(('catboost', CatBoostClassifier(iterations=100, \n",
    "                                        learning_rate=0.2,  \n",
    "                                        depth=6, \n",
    "                                        loss_function='Logloss', \n",
    "                                        eval_metric='AUC',  \n",
    "                                        random_state=42, verbose=False, allow_writing_files=False)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)  # Fit model to regular target\n",
    "        y_pred_reg = model.predict_proba(X_test)[:,1]\n",
    "        model.fit(X_train_high, y_train_high)  #Fit model to \n",
    "        y_pred_high = model.predict_proba(X_test)[:,1]\n",
    "        y_pred = reg_model_weight*y_pred_reg + high_model_weight*y_pred_high\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        top20 = avgCostSum(y_pred, y_test, X_test_raw)\n",
    "        results.append(\n",
    "        [name, auc, top20,top20/max_target])\n",
    "\n",
    "\n",
    "resdf = pd.DataFrame(results)\n",
    "resdf.columns = ['name', 'auc', 'top20' ,'%oftop20']\n",
    "resdf.groupby('name')[[\"auc\",\"top20\",\"%oftop20\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b5597-4848-4ef9-9192-ea42c4861179",
   "metadata": {},
   "source": [
    "<h4> Things that were tested:</h4>\n",
    "\n",
    "- Standardization: models perform better without standardizing\n",
    "- Standardizing + using PCA: Performs much worse\n",
    "- Adjusting the percentile: 90 works very well\n",
    "- Changing weight of high value targets. Couldn't find anything that works better than 0.8\n",
    "- RFW, XGB, and catboost all perform similarly.\n",
    "- Hard to find anything that pushes the %oftop20 above 0.7 on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0b177-b67a-4019-8e3f-4a52e69b7ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
